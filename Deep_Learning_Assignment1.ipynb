{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep-Learning-Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMooSMVFPai9VePVdsOYYrO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BlackCurrantDS/DeepLearning/blob/main/Deep_Learning_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OQeXHQZFKVq"
      },
      "source": [
        "Required Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVa3O7DWulqG"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import MNISTDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9eVFJlLunhD",
        "outputId": "fa791938-09a1-4fa0-a590-627310b18c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Getting MNIST data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "type(mnist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "module"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 362
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3rA96lTFg4L"
      },
      "source": [
        "#this loads data into train and test arrays\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKqTBv12F7Ua"
      },
      "source": [
        "#since its the pixels of images will use imshow library , seeing first 5 images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJuTdQCdFbZw",
        "outputId": "45e3b5c7-3d73-41e6-858b-27acbfe4cdb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(4):\n",
        "  print(\"The Training Label is \",train_labels[i] )\n",
        "  plt.imshow(train_images[i], cmap=\"Greys_r\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Training Label is  5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The Training Label is  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The Training Label is  4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The Training Label is  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjObjPujOoU8"
      },
      "source": [
        "Basics needed - \n",
        "\n",
        "1. Input Layers\n",
        "2. Output Layer\n",
        "3. Single Hidden Layer\n",
        "4. Function - Gonna use Softmax for hiddena and out\n",
        "5. Weights from input layer to hidden layer, hidden to out\n",
        "6. Bias from inout to hidden , hidden to out\n",
        "7. how many units in hidden layer?\n",
        "8. Loss fucntion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QJH52RySFdx"
      },
      "source": [
        "Just to keep a note to myself about Number of epoc, number of steps\n",
        "\n",
        "7770 rows, we break it into small batches of size 128, this will give us 60 batches.\n",
        "\n",
        "This will require tf to go through 60 iterations/steps to train to model. If we want to go over this dataset 10 times, set `num_epochs=10` will do the job, or, set `steps=600` should yield the same flow.\n",
        "\n",
        "Source- https://medium.com/@linda0511ny/tensorflow-train-dataset-by-epochs-or-steps-3839705f307d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxL0f-9vSigs"
      },
      "source": [
        "#Learning Rate is step size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41upmQoOZBic"
      },
      "source": [
        "data = MNISTDataset(train_images.reshape([-1, 784]), train_labels, \n",
        "                    test_images.reshape([-1, 784]), test_labels,\n",
        "                    batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pwm_u3CuthJ"
      },
      "source": [
        "train_steps = 1000\n",
        "learning_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGeCRXCrTLfd"
      },
      "source": [
        "#for hidden layer\n",
        "W1 = tf.Variable(np.zeros([784, 10]).astype(np.float32))\n",
        "b1 = tf.Variable(np.zeros(10, dtype=np.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89Ix7uXbaF-0",
        "outputId": "669a4cc2-ac44-484f-a9df-391f61efdb13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "#with single layer\n",
        "for step in range(train_steps):\n",
        "    img_batch, lbl_batch = data.next_batch()\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = tf.matmul(img_batch, W1) + b1\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logits, labels=lbl_batch))\n",
        "        \n",
        "    grads = tape.gradient(xent, [W1, b1])\n",
        "    W1.assign_sub(learning_rate * grads[0])\n",
        "    b1.assign_sub(learning_rate * grads[1])\n",
        "    \n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),\n",
        "                             tf.float32))\n",
        "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.3025853633880615 Accuracy: 0.0859375\n",
            "Loss: 0.5869016051292419 Accuracy: 0.84375\n",
            "Loss: 0.4638957381248474 Accuracy: 0.9140625\n",
            "Loss: 0.5068713426589966 Accuracy: 0.78125\n",
            "Loss: 0.36521804332733154 Accuracy: 0.921875\n",
            "Starting new epoch...\n",
            "Loss: 0.31971824169158936 Accuracy: 0.9375\n",
            "Loss: 0.3669887185096741 Accuracy: 0.890625\n",
            "Loss: 0.2975621819496155 Accuracy: 0.9140625\n",
            "Loss: 0.35932064056396484 Accuracy: 0.921875\n",
            "Loss: 0.335277795791626 Accuracy: 0.875\n",
            "Starting new epoch...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fltdm9aqaSq8",
        "outputId": "5b12f6b7-ed8c-4d86-a8db-38dac0360fd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_preds = tf.argmax(tf.matmul(data.test_data, W1) + b1, axis=1,\n",
        "                       output_type=tf.int32)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "                             tf.float32))\n",
        "print(\"Accuracy with single layer is\",acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy with single layer is tf.Tensor(0.911, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWIabIUmcpo0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD1Jn50oxhu9"
      },
      "source": [
        "#adding single hidden layer only and experimeting wiht number fof units in single hidden layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hFON5qtP_BK"
      },
      "source": [
        "#single laye with zeros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZQOID3WP_Mj",
        "outputId": "df547e12-a062-4d05-efbc-ee2af82fca55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "#for 1 hidden layer\n",
        "units = [784,512,350,256 ,128, 50, 10]\n",
        "for i in units:\n",
        "  W1 = tf.Variable(np.zeros([784, i],dtype=np.float32))\n",
        "  b1 = tf.Variable(np.zeros(i, dtype=np.float32))\n",
        "\n",
        "  #for out layer\n",
        "  W2 = tf.Variable(np.zeros([i, 10],dtype=np.float32))\n",
        "  b2 = tf.Variable(np.zeros(10, dtype=np.float32))\n",
        "\n",
        "  for step in range(train_steps):\n",
        "    img_batch, lbl_batch = data.next_batch()\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        logits1 = tf.matmul(img_batch, W1) + b1\n",
        "        logit = tf.matmul(logits1, W2) + b2\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logit, labels=lbl_batch))\n",
        "        \n",
        "        \n",
        "    grads1 = tape.gradient(xent, [W1, b1])\n",
        "    W1.assign_sub(learning_rate * grads1[0])\n",
        "    b1.assign_sub(learning_rate * grads1[1])\n",
        "\n",
        "      \n",
        "    grads2 = tape.gradient(xent, [W2, b2])\n",
        "    W2.assign_sub(learning_rate * grads2[0])\n",
        "    b2.assign_sub(learning_rate * grads2[1])\n",
        "\n",
        "  test_preds_layer_1 = tf.matmul(data.test_data, W1) + b1\n",
        "  test_preds = tf.argmax(tf.matmul(test_preds_layer_1, W2) + b2, axis=1,\n",
        "                       output_type=tf.int32)\n",
        "  acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "                             tf.float32))\n",
        "  print(\"Accuracy with {} units is {}\".format(i, acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 784 units is 0.11349999904632568\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 512 units is 0.11349999904632568\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 350 units is 0.11349999904632568\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 256 units is 0.11349999904632568\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 128 units is 0.11349999904632568\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 50 units is 0.11349999904632568\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 10 units is 0.11349999904632568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3rSkmG_yB-V",
        "outputId": "8cb6aaea-6ccc-4dd0-eb59-6b323f2c35b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "#for 1 hidden layer , but with uniform weights\n",
        "units = [784,512,350, 250,128, 50, 10]\n",
        "for i in units:\n",
        "  W1 = tf.Variable(tf.random.uniform([784, i], -0.1,0.1,dtype=np.float32))\n",
        "  b1 = tf.Variable(tf.random.uniform([i], -0.1,0.1, dtype=np.float32))\n",
        "\n",
        "  #for out layer\n",
        "  W2 = tf.Variable(tf.random.uniform([i, 10],-0.1,0.1,dtype=np.float32))\n",
        "  b2 = tf.Variable(tf.random.uniform([10],-0.1,0.1, dtype=np.float32))\n",
        "\n",
        "  for step in range(train_steps):\n",
        "    img_batch, lbl_batch = data.next_batch()\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        logits1 = tf.matmul(img_batch, W1) + b1\n",
        "        logit = tf.matmul(logits1, W2) + b2\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logit, labels=lbl_batch))\n",
        "        \n",
        "        \n",
        "    grads1 = tape.gradient(xent, [W1, b1])\n",
        "    W1.assign_sub(learning_rate * grads1[0])\n",
        "    b1.assign_sub(learning_rate * grads1[1])\n",
        "\n",
        "      \n",
        "    grads2 = tape.gradient(xent, [W2, b2])\n",
        "    W2.assign_sub(learning_rate * grads2[0])\n",
        "    b2.assign_sub(learning_rate * grads2[1])\n",
        "\n",
        "  test_preds_layer_1 = tf.matmul(data.test_data, W1) + b1\n",
        "  test_preds = tf.argmax(tf.matmul(test_preds_layer_1, W2) + b2, axis=1,\n",
        "                       output_type=tf.int32)\n",
        "  acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "                             tf.float32))\n",
        "  print(\"Accuracy with {} units is {}\".format(i, acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 784 units is 0.9174000024795532\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 512 units is 0.9162999987602234\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 350 units is 0.91839998960495\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 250 units is 0.9135000109672546\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 128 units is 0.914900004863739\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 50 units is 0.9146000146865845\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 10 units is 0.9132000207901001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTeYFY6HTaKJ",
        "outputId": "9a3d47e9-98ee-4db7-ff4c-be370d7d0115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "#for 1 hidden layer , but with uniform weights and sigmoid activation function on first layer\n",
        "units = [784,512,350,256, 128, 50, 10]\n",
        "for i in units:\n",
        "  W1 = tf.Variable(tf.random.uniform([784, i], -0.1,0.1,dtype=np.float32))\n",
        "  b1 = tf.Variable(tf.random.uniform([i], -0.1,0.1, dtype=np.float32))\n",
        "\n",
        "  #for out layer\n",
        "  W2 = tf.Variable(tf.random.uniform([i, 10],-0.1,0.1,dtype=np.float32))\n",
        "  b2 = tf.Variable(tf.random.uniform([10],-0.1,0.1, dtype=np.float32))\n",
        "\n",
        "  for step in range(train_steps):\n",
        "    img_batch, lbl_batch = data.next_batch()\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        logits1 = tf.nn.sigmoid(tf.matmul(img_batch, W1) + b1)\n",
        "        logit = tf.matmul(logits1, W2) + b2\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logit, labels=lbl_batch))\n",
        "        \n",
        "        \n",
        "    grads1 = tape.gradient(xent, [W1, b1])\n",
        "    W1.assign_sub(learning_rate * grads1[0])\n",
        "    b1.assign_sub(learning_rate * grads1[1])\n",
        "\n",
        "      \n",
        "    grads2 = tape.gradient(xent, [W2, b2])\n",
        "    W2.assign_sub(learning_rate * grads2[0])\n",
        "    b2.assign_sub(learning_rate * grads2[1])\n",
        "\n",
        "  test_preds_layer_1 = tf.nn.sigmoid(tf.matmul(data.test_data, W1) + b1)\n",
        "  test_preds = tf.argmax(tf.matmul(test_preds_layer_1, W2) + b2, axis=1,\n",
        "                       output_type=tf.int32)\n",
        "  acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "                             tf.float32))\n",
        "  print(\"Accuracy with {} units is {}\".format(i, acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 784 units is 0.9002000093460083\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 512 units is 0.8971999883651733\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 350 units is 0.8967000246047974\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 256 units is 0.8995000123977661\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 128 units is 0.8956000208854675\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 50 units is 0.8907999992370605\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 10 units is 0.8518999814987183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPhRtFY2ei6S",
        "outputId": "498ebc45-db93-43da-b4f2-e13a51833969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "#for 1 hidden layer , but with uniform weights and relu\n",
        "units = [784,512,350,256, 128, 50, 10]\n",
        "for i in units:\n",
        "  W1 = tf.Variable(tf.random.uniform([784, i], -0.1,0.1,dtype=np.float32))\n",
        "  b1 = tf.Variable(tf.random.uniform([i], -0.1,0.1, dtype=np.float32))\n",
        "\n",
        "  #for out layer\n",
        "  W2 = tf.Variable(tf.random.uniform([i, 10],-0.1,0.1,dtype=np.float32))\n",
        "  b2 = tf.Variable(tf.random.uniform([10],-0.1,0.1, dtype=np.float32))\n",
        "\n",
        "  for step in range(train_steps):\n",
        "    img_batch, lbl_batch = data.next_batch()\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        logits1 = tf.nn.relu(tf.matmul(img_batch, W1) + b1)\n",
        "        logit = tf.matmul(logits1, W2) + b2\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logit, labels=lbl_batch))\n",
        "       \n",
        "        \n",
        "    grads1 = tape.gradient(xent, [W1, b1])\n",
        "    W1.assign_sub(learning_rate * grads1[0])\n",
        "    b1.assign_sub(learning_rate * grads1[1])\n",
        "      \n",
        "    grads2 = tape.gradient(xent, [W2, b2])\n",
        "    W2.assign_sub(learning_rate * grads2[0])\n",
        "    b2.assign_sub(learning_rate * grads2[1])\n",
        "\n",
        "  test_preds_layer_1 = tf.nn.relu(tf.matmul(data.test_data, W1) + b1)\n",
        "  test_preds = tf.argmax(tf.matmul(test_preds_layer_1, W2) + b2, axis=1,\n",
        "                       output_type=tf.int32)\n",
        "  acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "                             tf.float32))\n",
        "  print(\"Accuracy with {} units is {}\".format(i, acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 784 units is 0.9448000192642212\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 512 units is 0.9401999711990356\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 350 units is 0.9387999773025513\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 256 units is 0.9379000067710876\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 128 units is 0.9336000084877014\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 50 units is 0.9301000237464905\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with 10 units is 0.9110000133514404\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOd_p1ZT1pl1",
        "outputId": "41ac443d-f0f4-49fd-d9d6-927f534dd7ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#adding one more layer, 2 hidden layer with softmax\n",
        "#for hidden layer\n",
        "units_i = [784,512,350,256, 128, 50, 10]\n",
        "units_j = [784,512,350,256, 128, 50, 10]\n",
        "for x in list(itertools.product(units_i,units_j)):\n",
        "  W1 = tf.Variable(tf.random.uniform([784, x[0]],-0.1,0.1,dtype=np.float32))\n",
        "  b1 = tf.Variable(tf.random.uniform([x[0]], -0.1,0.1,dtype=np.float32))\n",
        "\n",
        "  #for out layer\n",
        "  W2 = tf.Variable(tf.random.uniform([x[0], x[1]],-0.1,0.1,dtype=np.float32))\n",
        "  b2 = tf.Variable(tf.random.uniform([x[1]], -0.1,0.1,dtype=np.float32))\n",
        "\n",
        "  #for final layer\n",
        "  W3 = tf.Variable(tf.random.uniform([x[1], 10],-0.1,0.1,dtype=np.float32))\n",
        "  b3 = tf.Variable(tf.random.uniform([10], -0.1,0.1,dtype=np.float32))\n",
        "\n",
        "  for step in range(train_steps):\n",
        "    img_batch, lbl_batch = data.next_batch()\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        logits1 = tf.matmul(img_batch, W1) + b1\n",
        "        logit2 = tf.matmul(logits1, W2) + b2\n",
        "        logit3 = tf.matmul(logit2, W3) + b3\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logit3, labels=lbl_batch))\n",
        "        \n",
        "    grads1 = tape.gradient(xent, [W1, b1])\n",
        "    W1.assign_sub(learning_rate * grads1[0])\n",
        "    b1.assign_sub(learning_rate * grads1[1])\n",
        "      \n",
        "    grads2 = tape.gradient(xent, [W2, b2])\n",
        "    W2.assign_sub(learning_rate * grads2[0])\n",
        "    b2.assign_sub(learning_rate * grads2[1])\n",
        "\n",
        "      \n",
        "    grads3 = tape.gradient(xent, [W3, b3])\n",
        "    W3.assign_sub(learning_rate * grads3[0])\n",
        "    b3.assign_sub(learning_rate * grads3[1])\n",
        "\n",
        "  test_preds_layer_1 = tf.matmul(data.test_data, W1) + b1\n",
        "  test_preds_layer_2 = tf.matmul(test_preds_layer_1, W2) + b2\n",
        "  test_preds = tf.argmax(tf.matmul(test_preds_layer_2, W3) + b3, axis=1,\n",
        "                       output_type=tf.int32)\n",
        "  acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "                             tf.float32))\n",
        "  print(\"Accuracy with {} units is {}\".format(x, acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 784) units is 0.9175000190734863\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 512) units is 0.9136000275611877\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 350) units is 0.9133999943733215\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 256) units is 0.9190000295639038\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 128) units is 0.9111999869346619\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 50) units is 0.9154999852180481\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 10) units is 0.9168999791145325\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 784) units is 0.9174000024795532\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 512) units is 0.9197999835014343\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 350) units is 0.9180999994277954\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 256) units is 0.9189000129699707\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 128) units is 0.9151999950408936\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 50) units is 0.9146999716758728\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 10) units is 0.9154000282287598\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (350, 784) units is 0.9212999939918518\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (350, 512) units is 0.9172999858856201\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (350, 350) units is 0.9125999808311462\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (350, 256) units is 0.9153000116348267\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (350, 128) units is 0.9196000099182129\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (350, 50) units is 0.9154000282287598\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (350, 10) units is 0.916100025177002\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (256, 784) units is 0.9194999933242798\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (256, 512) units is 0.9053000211715698\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (256, 350) units is 0.9178000092506409\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (256, 256) units is 0.9168999791145325\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (256, 128) units is 0.9165999889373779\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (256, 50) units is 0.9186999797821045\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (256, 10) units is 0.9161999821662903\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 784) units is 0.9172999858856201\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 512) units is 0.9150999784469604\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 350) units is 0.9169999957084656\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 256) units is 0.9162999987602234\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 128) units is 0.9190999865531921\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 50) units is 0.9144999980926514\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 10) units is 0.9122999906539917\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 784) units is 0.9153000116348267\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 512) units is 0.9187999963760376\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 350) units is 0.9162999987602234\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 256) units is 0.9093000292778015\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 128) units is 0.9122999906539917\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 50) units is 0.9161999821662903\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 10) units is 0.910099983215332\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (10, 784) units is 0.9121000170707703\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (10, 512) units is 0.9157000184059143\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (10, 350) units is 0.9160000085830688\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (10, 256) units is 0.9161999821662903\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (10, 128) units is 0.9121999740600586\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (10, 50) units is 0.9031000137329102\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (10, 10) units is 0.8949000239372253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNBOt0B_vITf",
        "outputId": "f7bf11f6-dcf7-4fd5-e105-be392e4ad1b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#adding one more layer, 2 hidden layer with both sigmoid\n",
        "#for hidden layer\n",
        "units_i = [784,512,350,256, 128, 50, 10]\n",
        "units_j = [784,512,350,256, 128, 50, 10]\n",
        "for x in list(itertools.product(units_i,units_j)):\n",
        "  W1 = tf.Variable(tf.random.uniform([784, x[0]],-0.1,0.1,dtype=np.float32))\n",
        "  b1 = tf.Variable(tf.random.uniform([x[0]], -0.1,0.1,dtype=np.float32))\n",
        "\n",
        "  #for out layer\n",
        "  W2 = tf.Variable(tf.random.uniform([x[0], x[1]],-0.1,0.1,dtype=np.float32))\n",
        "  b2 = tf.Variable(tf.random.uniform([x[1]], -0.1,0.1,dtype=np.float32))\n",
        "\n",
        "  #for final layer\n",
        "  W3 = tf.Variable(tf.random.uniform([x[1], 10],-0.1,0.1,dtype=np.float32))\n",
        "  b3 = tf.Variable(tf.random.uniform([10], -0.1,0.1,dtype=np.float32))\n",
        "\n",
        "  for step in range(train_steps):\n",
        "    img_batch, lbl_batch = data.next_batch()\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        logits1 = tf.nn.sigmoid(tf.matmul(img_batch, W1) + b1)\n",
        "        logit2 = tf.nn.sigmoid(tf.matmul(logits1, W2) + b2)\n",
        "        logit3 = tf.matmul(logit2, W3) + b3\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logit3, labels=lbl_batch))\n",
        "        \n",
        "    grads1 = tape.gradient(xent, [W1, b1])\n",
        "    W1.assign_sub(learning_rate * grads1[0])\n",
        "    b1.assign_sub(learning_rate * grads1[1])\n",
        "      \n",
        "    grads2 = tape.gradient(xent, [W2, b2])\n",
        "    W2.assign_sub(learning_rate * grads2[0])\n",
        "    b2.assign_sub(learning_rate * grads2[1])\n",
        "\n",
        "      \n",
        "    grads3 = tape.gradient(xent, [W3, b3])\n",
        "    W3.assign_sub(learning_rate * grads3[0])\n",
        "    b3.assign_sub(learning_rate * grads3[1])\n",
        "\n",
        "  test_preds_layer_1 = tf.nn.sigmoid(tf.matmul(data.test_data, W1) + b1)\n",
        "  test_preds_layer_2 = tf.nn.sigmoid(tf.matmul(test_preds_layer_1, W2) + b2)\n",
        "  test_preds = tf.argmax(tf.matmul(test_preds_layer_2, W3) + b3, axis=1,\n",
        "                       output_type=tf.int32)\n",
        "  acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "                             tf.float32))\n",
        "  print(\"Accuracy with {} units is {}\".format(x, acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 784) units is 0.8955000042915344\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 512) units is 0.8927000164985657\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 350) units is 0.88919997215271\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 256) units is 0.8848000168800354\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 128) units is 0.8737999796867371\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 50) units is 0.8507999777793884\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 10) units is 0.7736999988555908\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 784) units is 0.8896999955177307\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 512) units is 0.8867999911308289\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 350) units is 0.8784000277519226\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 256) units is 0.878000020980835\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 128) units is 0.864300012588501\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 50) units is 0.8303999900817871\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 10) units is 0.6554999947547913\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (350, 784) units is 0.8838000297546387\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (350, 512) units is 0.8835999965667725\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (350, 350) units is 0.8665000200271606\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (350, 256) units is 0.8702999949455261\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (350, 128) units is 0.8507999777793884\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (350, 50) units is 0.8036999702453613\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (350, 10) units is 0.6187999844551086\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (256, 784) units is 0.8787000179290771\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (256, 512) units is 0.8781999945640564\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (256, 350) units is 0.870199978351593\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (256, 256) units is 0.8650000095367432\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (256, 128) units is 0.8429999947547913\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (256, 50) units is 0.7957000136375427\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (256, 10) units is 0.5378999710083008\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 784) units is 0.8701000213623047\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 512) units is 0.8629999756813049\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 350) units is 0.8425999879837036\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 256) units is 0.8299000263214111\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 128) units is 0.7980999946594238\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 50) units is 0.7063999772071838\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 10) units is 0.38350000977516174\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 784) units is 0.8309999704360962\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 512) units is 0.8377000093460083\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 350) units is 0.7929999828338623\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 256) units is 0.7505999803543091\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 128) units is 0.692799985408783\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 50) units is 0.5115000009536743\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 10) units is 0.14630000293254852\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (10, 784) units is 0.6995999813079834\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (10, 512) units is 0.5680000185966492\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (10, 350) units is 0.6827999949455261\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (10, 256) units is 0.5212000012397766\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (10, 128) units is 0.32989999651908875\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (10, 50) units is 0.25099998712539673\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (10, 10) units is 0.19220000505447388\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69XsPkrW7gZa",
        "outputId": "658ed986-c3c6-43c7-8d02-1d713813c5f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#adding one more layer, 3 hidden layer\n",
        "#for hidden layer\n",
        "units_i = [784,512, 128, 50, 10]\n",
        "units_j = [784,512, 128, 50, 10]\n",
        "units_k = [784,512, 128, 50, 10]\n",
        "for x in list(itertools.product(units_i,units_j,units_k)):\n",
        "  W1 = tf.Variable(tf.random.normal([784, x[0]],dtype=np.float32))\n",
        "  b1 = tf.Variable(np.zeros(x[0], dtype=np.float32))\n",
        "\n",
        "  #for out layer\n",
        "  W2 = tf.Variable(tf.random.normal([x[0], x[1]],dtype=np.float32))\n",
        "  b2 = tf.Variable(np.zeros(x[1], dtype=np.float32))\n",
        "\n",
        "  #for out layer\n",
        "  W3 = tf.Variable(tf.random.normal([x[1], x[2]],dtype=np.float32))\n",
        "  b3 = tf.Variable(np.zeros(x[2], dtype=np.float32))\n",
        "\n",
        "  #for final layer\n",
        "  W4 = tf.Variable(tf.random.normal([x[2], 10],dtype=np.float32))\n",
        "  b4 = tf.Variable(np.zeros(10, dtype=np.float32))\n",
        "\n",
        "  for step in range(train_steps):\n",
        "    img_batch, lbl_batch = data.next_batch()\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits1 = tf.matmul(img_batch, W1) + b1\n",
        "        xent1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logits1, labels=lbl_batch))\n",
        "        \n",
        "    grads1 = tape.gradient(xent1, [W1, b1])\n",
        "    W1.assign_sub(learning_rate * grads1[0])\n",
        "    b1.assign_sub(learning_rate * grads1[1])\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      logit2 = tf.matmul(tf.matmul(img_batch, W1) + b1, W2) + b2\n",
        "      xent2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logit2, labels=lbl_batch))\n",
        "      \n",
        "    grads2 = tape.gradient(xent2, [W2, b2])\n",
        "    W2.assign_sub(learning_rate * grads2[0])\n",
        "    b2.assign_sub(learning_rate * grads2[1])\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      logit3 = tf.matmul(logit2, W3) + b3\n",
        "      xent3 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logit3, labels=lbl_batch))\n",
        "      \n",
        "    grads3 = tape.gradient(xent3, [W3, b3])\n",
        "    W3.assign_sub(learning_rate * grads3[0])\n",
        "    b3.assign_sub(learning_rate * grads3[1])\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      logit4 = tf.matmul(logit3, W4) + b4\n",
        "      xent4 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logit4, labels=lbl_batch))\n",
        "      \n",
        "    grads4 = tape.gradient(xent4, [W4, b4])\n",
        "    W4.assign_sub(learning_rate * grads4[0])\n",
        "    b4.assign_sub(learning_rate * grads4[1])\n",
        "\n",
        "  test_preds_layer_1 = tf.matmul(data.test_data, W1) + b1\n",
        "  test_preds_layer_2 = tf.matmul(test_preds_layer_1, W2) + b2\n",
        "  test_preds_layer_3 = tf.matmul(test_preds_layer_2, W3) + b3\n",
        "  test_preds = tf.argmax(tf.matmul(test_preds_layer_3, W4) + b4, axis=1,\n",
        "                       output_type=tf.int32)\n",
        "  acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "                             tf.float32))\n",
        "  print(\"Accuracy with {} units is {}\".format(x, acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 784, 784) units is 0.7628999948501587\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 784, 512) units is 0.7745000123977661\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 784, 128) units is 0.6452999711036682\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 784, 50) units is 0.8434000015258789\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 784, 10) units is 0.8863999843597412\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 512, 784) units is 0.7609999775886536\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 512, 512) units is 0.7638999819755554\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 512, 128) units is 0.7767999768257141\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 512, 50) units is 0.7572000026702881\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 512, 10) units is 0.8406000137329102\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 128, 784) units is 0.8582000136375427\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 128, 512) units is 0.8697999715805054\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 128, 128) units is 0.6769999861717224\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 128, 50) units is 0.6929000020027161\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 128, 10) units is 0.8288000226020813\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 50, 784) units is 0.7760999798774719\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 50, 512) units is 0.7802000045776367\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 50, 128) units is 0.7174000144004822\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 50, 50) units is 0.6992999911308289\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 50, 10) units is 0.760200023651123\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 10, 784) units is 0.8033000230789185\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 10, 512) units is 0.8087000250816345\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 10, 128) units is 0.8633000254631042\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 10, 50) units is 0.6370000243186951\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (784, 10, 10) units is 0.7384999990463257\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 784, 784) units is 0.8532000184059143\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 784, 512) units is 0.8105000257492065\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 784, 128) units is 0.8715999722480774\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 784, 50) units is 0.7297000288963318\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 784, 10) units is 0.8087000250816345\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 512, 784) units is 0.5156000256538391\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 512, 512) units is 0.8481000065803528\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 512, 128) units is 0.8306000232696533\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 512, 50) units is 0.8105000257492065\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 512, 10) units is 0.7685999870300293\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 128, 784) units is 0.7505000233650208\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 128, 512) units is 0.8639000058174133\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 128, 128) units is 0.8084999918937683\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 128, 50) units is 0.758400022983551\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 128, 10) units is 0.8202000260353088\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 50, 784) units is 0.7735999822616577\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 50, 512) units is 0.8784000277519226\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 50, 128) units is 0.7730000019073486\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 50, 50) units is 0.7508000135421753\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 50, 10) units is 0.8034999966621399\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 10, 784) units is 0.7196000218391418\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 10, 512) units is 0.8033000230789185\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 10, 128) units is 0.784600019454956\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 10, 50) units is 0.829800009727478\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (512, 10, 10) units is 0.6431000232696533\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 784, 784) units is 0.7947999835014343\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 784, 512) units is 0.6783000230789185\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 784, 128) units is 0.703499972820282\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 784, 50) units is 0.6776000261306763\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 784, 10) units is 0.6413000226020813\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 512, 784) units is 0.7698000073432922\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 512, 512) units is 0.8381999731063843\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 512, 128) units is 0.7494999766349792\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 512, 50) units is 0.852400004863739\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 512, 10) units is 0.7490000128746033\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 128, 784) units is 0.7993999719619751\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 128, 512) units is 0.7670000195503235\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 128, 128) units is 0.8445000052452087\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 128, 50) units is 0.7782999873161316\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 128, 10) units is 0.7324000000953674\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 50, 784) units is 0.7490000128746033\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 50, 512) units is 0.7179999947547913\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 50, 128) units is 0.8434000015258789\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 50, 50) units is 0.7835000157356262\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 50, 10) units is 0.6337000131607056\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 10, 784) units is 0.7497000098228455\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 10, 512) units is 0.7944999933242798\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 10, 128) units is 0.7186999917030334\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 10, 50) units is 0.765500009059906\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (128, 10, 10) units is 0.7578999996185303\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 784, 784) units is 0.7141000032424927\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 784, 512) units is 0.7139999866485596\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 784, 128) units is 0.657800018787384\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 784, 50) units is 0.7929999828338623\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 784, 10) units is 0.6531000137329102\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 512, 784) units is 0.8014000058174133\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 512, 512) units is 0.7491000294685364\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 512, 128) units is 0.7055000066757202\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 512, 50) units is 0.7840999960899353\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Accuracy with (50, 512, 10) units is 0.7444000244140625\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-400-1fd6f44c67c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mgrads3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxent3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mW3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mW3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrads3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mb3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrads3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign_sub\u001b[0;34m(self, delta, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    799\u001b[0m       assign_sub_op = gen_resource_variable_ops.assign_sub_variable_op(\n\u001b[1;32m    800\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_sub_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_sub_variable_op\u001b[0;34m(resource, value, name)\u001b[0m\n\u001b[1;32m     92\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m     93\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AssignSubVariableOp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         tld.op_callbacks, resource, value)\n\u001b[0m\u001b[1;32m     95\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}